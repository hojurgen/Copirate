{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-ai-ml\n",
        "!pip install azure-identity\n",
        "!pip install mlflow==2.3.2\n",
        "!pip install scikit-learn==1.1\n",
        "!pip install pandas\n",
        "!pip install mlxtend\n",
        "!pip install azure-ai-ml mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721819924289
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Workspace\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "subscription_id = '<YourSubscriptionId>'\n",
        "resource_group = '<YourResourceGroup'\n",
        "workspace_name = 'YourWorkspace'\n",
        "\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace_name,\n",
        ")\n",
        "\n",
        "model_name = 'Apriori-ItemIds'\n",
        "compute_name = \"cpu-cluster\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721819925888
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.entities import Data\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "import mlflow\n",
        "import mlflow.pyfunc\n",
        "\n",
        "class AprioriModelWrapper(mlflow.pyfunc.PythonModel):\n",
        "    def __init__(self, frequent_itemsets, rules):\n",
        "        self.frequent_itemsets = frequent_itemsets\n",
        "        self.rules = rules\n",
        "\n",
        "    def predict(self, current_order):\n",
        "        relevant_rules = self.rules[[any(product in list(antecedents) for product in current_order) for antecedents in self.rules['antecedents']]]        \n",
        "        # Rank consequent items by support\n",
        "        next_items = relevant_rules.groupby('consequents').agg({'support': 'mean'}).sort_values(by='support', ascending=False)\n",
        "        \n",
        "        return next_items\n",
        "\n",
        "def main():\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"Path to the input file\")\n",
        "    parser.add_argument(\"--model_name\", type=str, help=\"Path to the input file\")\n",
        "    args = parser.parse_args()   \n",
        "\n",
        "    #Start an MLflow run\n",
        "    mlflow.start_run()\n",
        "\n",
        "    # Enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "    print(\"input data:\", args.data)\n",
        "\n",
        "    #transform data\n",
        "    df = pd.read_csv(args.data)\n",
        "\n",
        "    basket = df.pivot_table(index='salesid', columns='itemid', aggfunc='size', fill_value=0)\n",
        "    basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "    basket_bool = basket.astype(bool)\n",
        " \n",
        "    # Train the Apriori model\n",
        "    frequent_itemsets = apriori(basket_bool, min_support=0.01, use_colnames=True)\n",
        "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
        "\n",
        "    \n",
        "    # Log the frequent itemsets and rules as artifacts\n",
        "    frequent_itemsets.to_csv('frequent_itemsets.csv', index=False)\n",
        "    rules.to_csv('association_rules.csv', index=False)\n",
        "    mlflow.log_artifact(\"frequent_itemsets.csv\", artifact_path=args.model_name)\n",
        "    mlflow.log_artifact(\"association_rules.csv\", artifact_path=args.model_name)\n",
        "\n",
        "    # Log the model itself\n",
        "    mlflow.pyfunc.log_model(\n",
        "        artifact_path=args.model_name,\n",
        "        python_model=AprioriModelWrapper(frequent_itemsets, rules)\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pytorch-env.yml\n",
        "name: pytorch-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - scikit-learn=1.2.2\n",
        "  - pandas\n",
        "  - mlxtend\n",
        "  - azure-ai-ml\n",
        "  - pip\n",
        "  - pip:\n",
        "      - mlflow\n",
        "      - azureml-mlflow\n",
        "      - mltable\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient, command, Input\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "# set up the environment\n",
        "env = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    conda_file=\"pytorch-env.yml\",\n",
        "    name=\"pytorch-env\"\n",
        ")\n",
        "\n",
        "# register the environment\n",
        "ml_client.environments.create_or_update(env)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721819932329
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "# Create the compute cluster\n",
        "\n",
        "compute_cluster = AmlCompute(\n",
        "    name=compute_name,\n",
        "    size=\"STANDARD_D2_V2\",\n",
        "    min_instances=0,\n",
        "    max_instances=4\n",
        ")\n",
        "\n",
        "# Provision the compute cluster\n",
        "ml_client.compute.begin_create_or_update(compute_cluster).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721819963375
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input, command\n",
        "\n",
        "command_job = command(\n",
        "    inputs=dict(\n",
        "        data=Input(\n",
        "            type=\"uri_file\",\n",
        "            path=ml_client.data._get_latest_version(name=\"salesline\").path,\n",
        "        ),\n",
        "        model_name=model_name,\n",
        "    ),\n",
        "    code=\"./src\",\n",
        "    command=\"python train.py --data ${{inputs.data}} --model_name ${{inputs.model_name}}\",\n",
        "    environment=f\"{env.name}:{env.version}\",\n",
        "    compute=compute_name,\n",
        "    display_name=\"apriori-training-job\",\n",
        "    experiment_name=\"apriori-experiment\"\n",
        ")\n",
        "\n",
        "\n",
        "# Submit the job\n",
        "returned_job = ml_client.jobs.create_or_update(command_job)\n",
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721820406909
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model, Asset\n",
        "from azure.ai.ml.constants import ModelType\n",
        "\n",
        "# Register frequent_itemsets.csv as a model\n",
        "model = Model(\n",
        "    path=f'runs:/{returned_job.name}/{model_name}/',\n",
        "    name=f\"{model_name}\",\n",
        "    description=f\"{model_name} modelfor F&O data\",\n",
        "    type=\"mlflow_model\"\n",
        ")\n",
        "registered_model = ml_client.models.create_or_update(model)\n",
        "print(f\"Registered Model ID: {registered_model.id}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721820408337
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda.yml\n",
        "channels:\n",
        "  - defaults\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - pip\n",
        "  - pip:\n",
        "      - azureml-defaults\n",
        "      - azureml-contrib-services\n",
        "      - numpy\n",
        "      - pandas\n",
        "      - scikit-learn\n",
        "      - inference-schema[numpy,pandas]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient, command, Input\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "# Define the environment\n",
        "webservice_env = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    conda_file=\"conda.yaml\",\n",
        "    name=\"apriori_webservice_env\",\n",
        ")\n",
        "ml_client.environments.create_or_update(webservice_env)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721820415586
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile score.py\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from azureml.core.model import Model\n",
        "from inference_schema.schema_decorators import input_schema, output_schema\n",
        "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
        "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
        "\n",
        "logger = logging.getLogger('azureml.automl.core.scoring_script_v2')\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Sample input and output for schema\n",
        "sample_prediction = {\n",
        "    'antecedents': [\"ItemId1\", \"ItemId2\"],\n",
        "    'consequents': [\"RecommendedItemId1\", \"RecommendedItemId2\"],\n",
        "    'support': 0.123456789,\n",
        "    'confidence': 0.987654321,\n",
        "    'lift': 12.3456789012\n",
        "}\n",
        "\n",
        "sample_input = StandardPythonParameterType([\"ItemId1\", \"ItemId2\"])\n",
        "sample_output = StandardPythonParameterType({\n",
        "    \"predictions\": [sample_prediction]\n",
        "})\n",
        "\n",
        "def predict_next_items(current_basket, rules, top_n=5):\n",
        "    \"\"\"\n",
        "    Predicts the next items in the basket based on the current order using association rules.\n",
        "    \n",
        "    :param current_basket: list of items currently in the basket\n",
        "    :param rules: DataFrame containing the association rules\n",
        "    :param top_n: Number of top predictions to return\n",
        "    :return: DataFrame containing the top predictions with their respective metrics\n",
        "    \"\"\"\n",
        "    logger.info(f\"Current basket: {current_basket}\")\n",
        "    logger.info(f\"Total number of rules: {len(rules)}\")\n",
        "    \n",
        "    # Filter rules where all items in the antecedents are in the current basket\n",
        "    applicable_rules = rules[rules['antecedents'].apply(lambda x: all(item in current_basket for item in x))]\n",
        "    \n",
        "    logger.info(f\"Number of applicable rules: {len(applicable_rules)}\")\n",
        "    \n",
        "    if applicable_rules.empty:\n",
        "        return pd.DataFrame(columns=['antecedents', 'consequents', 'support', 'confidence', 'lift'])\n",
        "\n",
        "    # Sort rules by confidence and lift\n",
        "    applicable_rules = applicable_rules.sort_values(by=['confidence', 'lift'], ascending=False)\n",
        "\n",
        "    # Select top N rules\n",
        "    top_rules = applicable_rules.head(top_n)\n",
        "    \n",
        "    # Extract the consequents\n",
        "    predictions = top_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
        "    \n",
        "    # Convert frozensets to lists for JSON serialization\n",
        "    predictions['antecedents'] = predictions['antecedents'].apply(list)\n",
        "    predictions['consequents'] = predictions['consequents'].apply(list)\n",
        "    \n",
        "    logger.info(f\"Top predictions: {predictions}\")\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "def init():\n",
        "    global rules\n",
        "    # Load the model\n",
        "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"Apriori-ItemIds\")\n",
        "    model_path += \"/association_rules.csv\"\n",
        "    rules = pd.read_csv(model_path)\n",
        "    logger.info(f\"Loaded rules from {model_path}\")\n",
        "    logger.info(f\"Rules sample: {rules.head()}\")\n",
        "    # Ensure the 'antecedents' and 'consequents' columns are properly processed if they are string representations of lists\n",
        "    if rules['antecedents'].dtype == object:\n",
        "        rules['antecedents'] = rules['antecedents'].apply(lambda x: x.strip(\"frozenset({})\").replace(\"'\", \"\").replace(\" \", \"\").split(\",\"))\n",
        "    if rules['consequents'].dtype == object:\n",
        "        rules['consequents'] = rules['consequents'].apply(lambda x: x.strip(\"frozenset({})\").replace(\"'\", \"\").replace(\" \", \"\").split(\",\"))\n",
        "    logger.info(f\"Processed rules sample: {rules.head()}\")\n",
        "\n",
        "@input_schema('Inputs', sample_input)\n",
        "@output_schema(sample_output)\n",
        "def run(Inputs):\n",
        "    try:\n",
        "        # Parse input data\n",
        "        current_basket = Inputs\n",
        "\n",
        "        # Debugging: Log the input data\n",
        "        logger.info(f\"Received input: {Inputs}\")\n",
        "\n",
        "        # Get the top predictions\n",
        "        top_predictions = predict_next_items(current_basket, rules, top_n=5)\n",
        "\n",
        "        # Debugging: Log the number of predictions found\n",
        "        logger.info(f\"Number of predictions found: {len(top_predictions)}\")\n",
        "\n",
        "        # Convert predictions to JSON format\n",
        "        predictions_json = top_predictions.to_json(orient=\"records\")\n",
        "\n",
        "        # Format the response as required\n",
        "        response = {\n",
        "            \"predictions\": json.loads(predictions_json)\n",
        "        }\n",
        "\n",
        "        # Return the formatted response\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        # Log the exception for debugging\n",
        "        logger.error(f\"Exception occurred: {str(e)}\")\n",
        "        return str(e)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import CodeConfiguration\n",
        "\n",
        "# Define the inference configuration\n",
        "inference_config = CodeConfiguration(\n",
        "    code=\"\",\n",
        "    scoring_script=\"score.py\"\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721824436046
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Define the endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=f\"{model_name}-endpoint\".lower(),\n",
        "    description=f\"Endpoint for {model_name} model\",\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721824442601
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "#Get Model Name and version from previous job \n",
        "#models = ml_client.models.list(name=registered_model.name)\n",
        "#highest_model_version = registered_model.version\n",
        "\n",
        "#alternative get name and highest version\n",
        "models = ml_client.models.list(name=model_name)\n",
        "highest_model_version = max(models, key=lambda model: model.version)\n",
        "\n",
        "# Define the deployment\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=f\"{model_name}-v{highest_model_version.version}-deployment\",\n",
        "    endpoint_name=endpoint.name,\n",
        "    model=highest_model_version, \n",
        "    environment=webservice_env,\n",
        "    code_configuration=inference_config,\n",
        "    instance_type=\"Standard_DS3_v2\",\n",
        "    instance_count=1\n",
        ")\n",
        "\n",
        "webservice_deployment = ml_client.online_deployments.begin_create_or_update(deployment)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721824524788
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the endpoint to route traffic to the deployment\n",
        "endpoint.traffic = {deployment.name: 100}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721829261505
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import os\n",
        "import ssl\n",
        "\n",
        "def allowSelfSignedHttps(allowed):\n",
        "    # bypass the server certificate verification on client side\n",
        "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
        "\n",
        "# Request data goes here\n",
        "# The example below assumes JSON formatting which may be updated\n",
        "# depending on the format your endpoint expects.\n",
        "# More information can be found here:\n",
        "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
        "data = {\"Inputs\": ['D0001',\"L0001\"]} \n",
        "\n",
        "body = str.encode(json.dumps(data))\n",
        "\n",
        "url = '<your-Endpoint>/score'\n",
        "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
        "api_key = '<your Api Key>'\n",
        "if not api_key:\n",
        "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
        "\n",
        "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
        "# Remove this header to have the request observe the endpoint traffic rules\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'apriori-itemids-v1-deployment' }\n",
        "\n",
        "req = urllib.request.Request(url, body, headers)\n",
        "\n",
        "print(('Bearer '+ api_key))\n",
        "\n",
        "try:\n",
        "    response = urllib.request.urlopen(req)\n",
        "\n",
        "    result = response.read()\n",
        "    print(result)\n",
        "except urllib.error.HTTPError as error:\n",
        "    print(\"The request failed with status code: \" + str(error.code))\n",
        "\n",
        "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
        "    print(error.info())\n",
        "    print(error.read().decode(\"utf8\", 'ignore'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721829659182
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "swagger_url = \"<your-Endpoint>/swagger.json\"\n",
        "api_key = '<your Api Key>'\n",
        "# Headers for the request\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "}\n",
        "\n",
        "# Make the GET request to the Swagger URL\n",
        "response = requests.get(swagger_url, headers=headers)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    swagger_json = response.json()\n",
        "    \n",
        "    # Save the Swagger JSON to a file\n",
        "    with open('swagger.json', 'w') as json_file:\n",
        "        json.dump(swagger_json, json_file, indent=4)\n",
        "    print(\"Swagger documentation downloaded successfully.\")\n",
        "else:\n",
        "    print(f\"Failed to download Swagger documentation. Status code: {response.status_code}\")\n",
        "    print(f\"Response: {response.text}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721829310515
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}